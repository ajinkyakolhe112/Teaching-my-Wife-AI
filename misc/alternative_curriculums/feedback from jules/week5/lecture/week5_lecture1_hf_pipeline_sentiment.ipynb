{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5.1: Sentiment Analysis with Hugging Face Pipelines and Pre-trained Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Hugging Face `transformers` library offers a high-level abstraction called `pipeline` that makes it incredibly easy to use pre-trained models for various NLP tasks, including sentiment analysis. Pipelines encapsulate the entire process: loading a pre-trained model and its tokenizer, pre-processing input text, running the text through the model, and post-processing the model's output into a human-readable format.\n",
    "\n",
    "This notebook will demonstrate how to use the `sentiment-analysis` pipeline, explore its components, and understand what happens under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Usage of the Sentiment Analysis Pipeline\n",
    "\n",
    "Using the `pipeline` function for sentiment analysis is straightforward. If you don't specify a model, it defaults to a pre-trained model fine-tuned for sentiment analysis (often DistilBERT SST-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure transformers library is installed\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "# If no model is specified, it uses a default model for this task.\n",
    "# The first time you run this, it will download the default model and tokenizer.\n",
    "classifier = pipeline(task=\"sentiment-analysis\")\n",
    "\n",
    "# Prepare some example texts\n",
    "texts = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"The movie was okay, not great but not terrible either.\",\n",
    "    \"This is an absolutely fantastic experience!\"\n",
    "]\n",
    "\n",
    "# Get predictions\n",
    "predictions = classifier(texts)\n",
    "\n",
    "# Print the predictions\n",
    "for text, prediction in zip(texts, predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {prediction['label']}, Score: {prediction['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Specifying a Model for the Pipeline\n",
    "\n",
    "You can choose a specific model from the Hugging Face Hub to use within the pipeline. For example, `nlptown/bert-base-multilingual-uncased-sentiment` is a model fine-tuned for sentiment analysis on product reviews in multiple languages, and it can output ratings from 1 to 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Specify a different model for sentiment analysis\n",
    "# This model predicts star ratings (1 to 5 stars)\n",
    "specific_classifier = pipeline(\n",
    "    task=\"sentiment-analysis\", \n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "reviews = [\n",
    "    \"This product is amazing, exceeded all my expectations!\", # English\n",
    "    \"C'est un désastre complet, ne l'achetez pas.\", # French (This is a complete disaster, don't buy it.)\n",
    "    \"El servicio fue terrible y la comida peor.\", # Spanish (The service was terrible and the food worse.)\n",
    "    \"Ganz gut, aber nichts besonderes.\", # German (Quite good, but nothing special.)\n",
    "    \"まあまあです\" # Japanese (It's so-so / okay)\n",
    "]\n",
    "\n",
    "star_predictions = specific_classifier(reviews)\n",
    "\n",
    "for review, prediction in zip(reviews, star_predictions):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Rating: {prediction['label']}, Score: {prediction['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploring the Pipeline Object\n",
    "\n",
    "A pipeline object (like our `classifier` or `specific_classifier`) bundles together the model, tokenizer, and other configuration. Let's inspect the components of the default classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default classifier from the first example\n",
    "print(f\"Task: {classifier.task}\") \n",
    "print(f\"Model: {type(classifier.model)}\")\n",
    "print(f\"Tokenizer: {type(classifier.tokenizer)}\")\n",
    "print(f\"Device: {classifier.device}\")\n",
    "\n",
    "# You can print the model itself to see its architecture\n",
    "# print(\"\\nModel Architecture:\")\n",
    "# print(classifier.model)\n",
    "\n",
    "# And the tokenizer configuration\n",
    "# print(\"\\nTokenizer Configuration:\")\n",
    "# print(classifier.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default model for `sentiment-analysis` is often `distilbert-base-uncased-finetuned-sst-2-english`. \n",
    "This model has:\n",
    "*   A **tokenizer** (`DistilBertTokenizerFast`) to convert text into numerical IDs.\n",
    "*   A **model** (`DistilBertForSequenceClassification`) which is a DistilBERT transformer architecture with a sequence classification head on top.\n",
    "\n",
    "Let's break down the process manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Tokenization\n",
    "\n",
    "The tokenizer first converts text into tokens (words, sub-words, or symbols) and then maps these tokens to numerical IDs. It also adds special tokens required by the model (like `[CLS]` and `[SEP]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer used by the default sentiment analysis pipeline\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" # Default model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "# Tokenize the inputs\n",
    "# padding=True ensures all sequences in a batch have the same length.\n",
    "# truncation=True cuts sequences longer than the model's max length.\n",
    "# return_tensors=\"pt\" returns PyTorch tensors.\n",
    "tokenized_inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Tokenized Inputs:\")\n",
    "for key, value in tokenized_inputs.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nShape of input_ids: {tokenized_inputs['input_ids'].shape}\")\n",
    "\n",
    "# You can decode the input_ids back to tokens to see what they look like\n",
    "print(f\"\\nTokens for the first sentence: {tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Model Inference\n",
    "\n",
    "The tokenized inputs are then passed to the model. The model outputs \"logits,\" which are raw, unnormalized scores for each class (e.g., positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Move inputs to the same device as the model (e.g., GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs_on_device = {k: v.to(device) for k, v in tokenized_inputs.items()}\n",
    "\n",
    "# Get model outputs (logits)\n",
    "with torch.no_grad(): # Disable gradient calculations for inference\n",
    "    outputs = model(**inputs_on_device)\n",
    "\n",
    "print(\"Model Outputs (Logits):\")\n",
    "print(outputs.logits)\n",
    "print(f\"\\nShape of logits: {outputs.logits.shape}\") # (batch_size, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output `logits` are raw scores. For the first sentence (\"I've been waiting...\"), the logits might be `[-1.5607,  1.6123]`. For the second (\"I hate this...\"), they might be `[ 3.1931, -2.6685]`.\n",
    "\n",
    "These represent the model's confidence for 'NEGATIVE' (first value) and 'POSITIVE' (second value) respectively, before normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Post-processing (Softmax)\n",
    "\n",
    "To convert these logits into probabilities, a Softmax function is applied. The pipeline does this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "print(\"Probabilities:\")\n",
    "print(probabilities)\n",
    "\n",
    "# Get the predicted label ID (0 for 'NEGATIVE', 1 for 'POSITIVE' in this model's config)\n",
    "predicted_label_ids = torch.argmax(probabilities, dim=-1)\n",
    "print(f\"\\nPredicted Label IDs: {predicted_label_ids}\")\n",
    "\n",
    "# Map label IDs to human-readable labels (from the model's configuration)\n",
    "labels = [model.config.id2label[label_id] for label_id in predicted_label_ids.tolist()]\n",
    "print(f\"Predicted Labels: {labels}\")\n",
    "\n",
    "# The pipeline also returns the score of the predicted class\n",
    "scores = torch.max(probabilities, dim=-1).values\n",
    "print(f\"Scores for predicted labels: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This manual breakdown shows the steps that the `pipeline` function handles automatically:\n",
    "1.  **Tokenization:** Text to input IDs.\n",
    "2.  **Inference:** Input IDs to logits.\n",
    "3.  **Post-processing:** Logits to probabilities and then to human-readable labels with scores.\n",
    "\n",
    "This makes pipelines very convenient for quick application of pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hugging Face Pipelines provide a very simple and effective way to use pre-trained models for a variety of NLP tasks, including sentiment analysis. They abstract away much of the boilerplate code for loading models, tokenizing text, performing inference, and processing the results.\n",
    "\n",
    "Key Takeaways:\n",
    "*   Pipelines are great for **quick prototyping** and applying models with minimal code.\n",
    "*   You can use the **default model** for a task or specify any compatible model from the Hugging Face Hub.\n",
    "*   Under the hood, pipelines perform **tokenization, model inference (getting logits), and post-processing (like softmax and label mapping)**.\n",
    "*   Understanding these underlying steps can be helpful when you need more control or want to build custom model interaction logic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
