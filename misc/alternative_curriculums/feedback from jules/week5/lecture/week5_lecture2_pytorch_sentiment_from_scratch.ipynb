{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5.2: Building a Simple Sentiment Analysis Model from Scratch with PyTorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll walk through the process of building, training, and evaluating a simple sentiment analysis model using PyTorch. This approach gives us more control over the model architecture and training process compared to using pre-built pipelines. We'll be examining the components from two Python scripts:\n",
    "\n",
    "1.  `simple_sentiment_model_pytorch.py`: Defines the tokenizer, the PyTorch model class, and data loading/preparation functions.\n",
    "2.  `train_sentiment_model_pytorch.py`: Contains the training loop, evaluation logic, and utilities for saving the model.\n",
    "\n",
    "While the scripts are designed to be run directly, we will replicate parts of their logic here to explain each component step-by-step.\n",
    "\n",
    "**Learning Objectives:**\n",
    "*   Understand how to create a custom tokenizer.\n",
    "*   Learn to define a simple neural network for text classification in PyTorch.\n",
    "*   See how to prepare a dataset and use PyTorch `DataLoader`.\n",
    "*   Grasp the fundamentals of a PyTorch training loop.\n",
    "\n",
    "**Note:** For this notebook, we'll focus on explaining the code. To run the full training, you would typically execute the `train_sentiment_model_pytorch.py` script in an environment with PyTorch, datasets, and tqdm installed (`pip install torch datasets tqdm scikit-learn pandas`). Weights and Biases (`wandb`) is also used for logging in the script, which is optional but good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The `SimpleTokenizer` Class\n",
    "\n",
    "Source: `simple_sentiment_model_pytorch.py`\n",
    "\n",
    "The `SimpleTokenizer` is responsible for converting raw text into a sequence of numerical tokens that our model can understand. It performs several key steps:\n",
    "\n",
    "1.  **Vocabulary Building (`build_vocab`):**\n",
    "    *   Takes a list of texts as input.\n",
    "    *   Cleans the text: converts to lowercase, removes special characters (keeps alphanumeric and spaces).\n",
    "    *   Counts the frequency of all words in the corpus.\n",
    "    *   Builds a vocabulary (`word2idx` mapping words to integer indices) using the most common words, up to `max_vocab_size`.\n",
    "    *   Includes special tokens: `<pad>` (for padding shorter sequences) and `<unk>` (for unknown words not in the vocabulary).\n",
    "\n",
    "2.  **Tokenization (`tokenize`):**\n",
    "    *   Takes a single text string as input.\n",
    "    *   Cleans the text (lowercase, remove special characters).\n",
    "    *   Splits the text into words.\n",
    "    *   Converts words to their corresponding integer indices using the built vocabulary. Uses `<unk>` for out-of-vocabulary words.\n",
    "    *   Pads or truncates the sequence of indices to a fixed `max_length`.\n",
    "    *   Returns a PyTorch tensor of these indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, max_length=250):\n",
    "        self.max_length = max_length\n",
    "        self.word2idx = {} # Dictionary to map words to unique integer IDs\n",
    "        self.idx2word = {} # Dictionary to map IDs back to words\n",
    "        \n",
    "    def build_vocab(self, texts, max_vocab_size=10000):\n",
    "        \"\"\"Builds a vocabulary from a list of texts.\"\"\"\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            # Clean and tokenize text\n",
    "            text = text.lower() # Convert to lowercase\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuation, keep words and spaces\n",
    "            words = text.split() # Split by whitespace\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Get the most common words, reserving space for <pad> and <unk> tokens\n",
    "        most_common = word_counts.most_common(max_vocab_size - 2)\n",
    "        \n",
    "        # Initialize vocabulary with special tokens\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
    "        for word, _ in most_common:\n",
    "            if word not in self.word2idx: # Ensure no duplicates if a common word is somehow a special token name\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "        \n",
    "        # Create reverse mapping from index to word\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Converts a single text string to a tensor of token indices.\"\"\"\n",
    "        # Convert to lowercase and remove special characters\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Convert words to indices, using <unk> for words not in vocab\n",
    "        indices = [self.word2idx.get(token, self.word2idx['<unk>']) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length] # Truncate\n",
    "        else:\n",
    "            indices = indices + [self.word2idx['<pad>']] * (self.max_length - len(indices)) # Pad\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "# Example Usage of SimpleTokenizer\n",
    "sample_texts = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document? Yes, it is.\"\n",
    "]\n",
    "\n",
    "tokenizer = SimpleTokenizer(max_length=10)\n",
    "tokenizer.build_vocab(sample_texts, max_vocab_size=10) # Small vocab for demo\n",
    "\n",
    "print(f\"Vocabulary (word2idx): {tokenizer.word2idx}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word2idx)}\")\n",
    "\n",
    "example_sentence = \"This is a new document, is it not?\"\n",
    "tokenized_sentence = tokenizer.tokenize(example_sentence)\n",
    "\n",
    "print(f\"\\nOriginal sentence: {example_sentence}\")\n",
    "print(f\"Tokenized indices: {tokenized_sentence}\")\n",
    "print(f\"Reconstructed tokens: {[tokenizer.idx2word[idx.item()] for idx in tokenized_sentence]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The `SimpleSentimentModel` Class\n",
    "\n",
    "Source: `simple_sentiment_model_pytorch.py`\n",
    "\n",
    "This PyTorch `nn.Module` defines a simple neural network for sentiment classification. Its architecture is:\n",
    "\n",
    "1.  **Embedding Layer (`nn.Embedding`):**\n",
    "    *   Takes the vocabulary size and an embedding dimension as input.\n",
    "    *   Maps each token index to a dense vector representation (embedding).\n",
    "    *   These embeddings are learned during training.\n",
    "    *   Output shape: `(batch_size, sequence_length, embedding_dim)`.\n",
    "\n",
    "2.  **Average Pooling:**\n",
    "    *   The model uses `torch.mean(x, dim=1)` to average the embeddings across the `sequence_length` dimension.\n",
    "    *   This creates a single fixed-size vector representing the entire input sequence.\n",
    "    *   Output shape: `(batch_size, embedding_dim)`.\n",
    "    *   *Note: More complex models might use LSTMs, GRUs, or Transformer encoders here instead of simple averaging.*\n",
    "\n",
    "3.  **First Fully Connected Layer (`nn.Linear`):**\n",
    "    *   Takes the `embedding_dim` as input and outputs 32 features.\n",
    "    *   Followed by a ReLU activation function (`nn.ReLU`) to introduce non-linearity.\n",
    "\n",
    "4.  **Second Fully Connected Layer (`nn.Linear`):**\n",
    "    *   Takes the 32 features from the previous layer and outputs a single logit.\n",
    "    *   This logit represents the model's prediction for the sentiment (e.g., a higher value might indicate positive sentiment, a lower value negative, before applying a sigmoid for binary classification).\n",
    "\n",
    "The `forward` method defines how input `x` (a batch of tokenized sequences) flows through these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=32):\n",
    "        super(SimpleSentimentModel, self).__init__()\n",
    "        # Embedding layer: Converts token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) # padding_idx=0 tells the model to ignore <pad> tokens for learning embeddings\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Output layer: A single neuron for binary sentiment (positive/negative)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        \n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Average pooling: Take the mean of word embeddings across the sequence length dimension\n",
    "        # This creates a single vector representation for the entire sequence.\n",
    "        pooled = torch.mean(embedded, dim=1)  # Shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # Pass through the first fully connected layer and ReLU activation\n",
    "        out_fc1 = self.relu(self.fc1(pooled)) # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Pass through the output layer to get logits\n",
    "        logits = self.fc2(out_fc1) # Shape: (batch_size, 1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Example instantiation of the model\n",
    "VOCAB_SIZE_EXAMPLE = len(tokenizer.word2idx) # From our previous tokenizer example\n",
    "EMBEDDING_DIM_EXAMPLE = 50\n",
    "HIDDEN_DIM_EXAMPLE = 16\n",
    "\n",
    "example_model = SimpleSentimentModel(VOCAB_SIZE_EXAMPLE, EMBEDDING_DIM_EXAMPLE, HIDDEN_DIM_EXAMPLE)\n",
    "print(example_model)\n",
    "\n",
    "# Test with a dummy batch of tokenized sentences\n",
    "dummy_batch_size = 2\n",
    "dummy_input_indices = torch.stack([\n",
    "    tokenizer.tokenize(\"this is a good movie\"), \n",
    "    tokenizer.tokenize(\"a bad film experience\")\n",
    "]) # Shape: (2, max_length)\n",
    "\n",
    "print(f\"\\nDummy input shape: {dummy_input_indices.shape}\")\n",
    "with torch.no_grad(): # No need to calculate gradients for this example\n",
    "    dummy_output = example_model(dummy_input_indices)\n",
    "print(f\"Dummy output logits shape: {dummy_output.shape}\") # Expected: (batch_size, 1)\n",
    "print(f\"Dummy output logits: \\n{dummy_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The `load_and_prepare_data` Function\n",
    "\n",
    "Source: `simple_sentiment_model_pytorch.py`\n",
    "\n",
    "This function handles loading the IMDB dataset (a common benchmark for sentiment analysis) and preparing it for training.\n",
    "\n",
    "1.  **Load Dataset:** Uses the `datasets` library from Hugging Face to download and load the `stanfordnlp/imdb` dataset. This dataset contains movie reviews labeled as positive (1) or negative (0).\n",
    "2.  **Initialize Tokenizer & Build Vocabulary:** Creates an instance of `SimpleTokenizer` and builds its vocabulary using the training texts from the IMDB dataset.\n",
    "3.  **Convert Texts to Tensors:** Tokenizes all training texts and converts their labels into PyTorch tensors.\n",
    "4.  **Create DataLoader:** Uses `torch.utils.data.DataLoader` to create an iterable that provides batches of data (tokenized texts and labels) during training. This helps manage memory and provides options for shuffling and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset # Hugging Face datasets library\n",
    "\n",
    "def load_and_prepare_data(batch_size=32, max_vocab_size=10000, max_seq_length=250, subset_size=None):\n",
    "    \"\"\"Loads the IMDB dataset, tokenizes, and prepares DataLoader.\"\"\"\n",
    "    print(\"Loading IMDB dataset...\")\n",
    "    # Load IMDB dataset (train split only for this example)\n",
    "    dataset = load_dataset(\"stanfordnlp/imdb\", split='train')\n",
    "    \n",
    "    if subset_size:\n",
    "        print(f\"Using a subset of {subset_size} examples for faster demonstration.\")\n",
    "        dataset = dataset.select(range(subset_size))\n",
    "        \n",
    "    train_texts = dataset['text']\n",
    "    train_labels = dataset['label']\n",
    "    \n",
    "    print(\"Building tokenizer vocabulary...\")\n",
    "    # Initialize tokenizer and build vocabulary\n",
    "    local_tokenizer = SimpleTokenizer(max_length=max_seq_length)\n",
    "    local_tokenizer.build_vocab(train_texts, max_vocab_size=max_vocab_size)\n",
    "    \n",
    "    print(\"Tokenizing texts...\")\n",
    "    # Convert texts to tensors\n",
    "    # This can be memory intensive for large datasets if done all at once\n",
    "    train_data_tensors = torch.stack([local_tokenizer.tokenize(text) for text in train_texts])\n",
    "    train_labels_tensors = torch.tensor(train_labels, dtype=torch.float) # BCEWithLogitsLoss expects float labels\n",
    "    \n",
    "    print(\"Creating DataLoader...\")\n",
    "    # Create DataLoader directly from tensors\n",
    "    # In PyTorch, a Dataset object is often used here, but for simplicity, we use a list of tuples.\n",
    "    train_dataset_for_loader = list(zip(train_data_tensors, train_labels_tensors))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset_for_loader,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"Data loading and preparation complete.\")\n",
    "    return train_loader, local_tokenizer\n",
    "\n",
    "# Example of using load_and_prepare_data (with a small subset for speed)\n",
    "# Note: Running this will download the IMDB dataset if you haven't already (approx. 84MB)\n",
    "try:\n",
    "    # Using a small subset to speed up the demonstration in the notebook\n",
    "    # For actual training, you'd use a larger portion or the whole dataset.\n",
    "    example_train_loader, example_tokenizer_for_data = load_and_prepare_data(batch_size=4, subset_size=20) \n",
    "    \n",
    "    print(f\"\\nVocabulary size from data: {len(example_tokenizer_for_data.word2idx)}\")\n",
    "    print(f\"Number of training examples (subset): {len(example_train_loader.dataset)}\")\n",
    "    print(f\"Number of batches: {len(example_train_loader)}\")\n",
    "    \n",
    "    # Inspect a batch\n",
    "    for inputs, labels in example_train_loader:\n",
    "        print(\"\\nSample batch:\")\n",
    "        print(f\"Inputs shape: {inputs.shape}\") # (batch_size, max_seq_length)\n",
    "        print(f\"Labels shape: {labels.shape}\") # (batch_size)\n",
    "        print(f\"First input sequence: {inputs[0]}\")\n",
    "        print(f\"First label: {labels[0]}\")\n",
    "        break # Only show one batch\n",
    "except Exception as e:\n",
    "    print(f\"Could not load data (อาจจะต้องต่อเน็ต or check dataset availability): {e}\")\n",
    "    print(\"Skipping data loading demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The `train_model` Function\n",
    "\n",
    "Source: `train_sentiment_model_pytorch.py`\n",
    "\n",
    "This function orchestrates the training process for our `SimpleSentimentModel`.\n",
    "\n",
    "1.  **Set Model to Train Mode:** `model.train()` tells PyTorch that the model is in training mode (this enables features like dropout if used).\n",
    "2.  **Epoch Loop:** Iterates for a specified number of `num_epochs`.\n",
    "3.  **Batch Loop:** Uses `tqdm` for a progress bar while iterating through batches from the `train_loader`.\n",
    "    *   **Move Data to Device:** Moves input tensors and labels to the specified `device` (CPU or GPU).\n",
    "    *   **Zero Gradients:** `optimizer.zero_grad()` clears old gradients before calculating new ones.\n",
    "    *   **Forward Pass:** `outputs = model(inputs)` gets the model's predictions (logits).\n",
    "    *   **Calculate Loss:** `loss = criterion(outputs.squeeze(), labels)` computes the loss. `nn.BCEWithLogitsLoss` is used, which combines a Sigmoid layer and Binary Cross Entropy loss, suitable for binary classification. It expects raw logits.\n",
    "    *   **Backward Pass:** `loss.backward()` computes the gradients of the loss with respect to model parameters.\n",
    "    *   **Optimizer Step:** `optimizer.step()` updates the model parameters using the calculated gradients.\n",
    "    *   **Track Statistics:** Accumulates loss and calculates accuracy for the batch and epoch.\n",
    "    *   **Logging (W&B):** If Weights & Biases (`wandb`) is used (as in the script), it logs batch and epoch metrics.\n",
    "\n",
    "The `main()` function in `train_sentiment_model_pytorch.py` handles:\n",
    "*   Initializing `wandb` (optional).\n",
    "*   Setting the device.\n",
    "*   Calling `load_and_prepare_data`.\n",
    "*   Initializing the `SimpleSentimentModel` and moving it to the device.\n",
    "*   Defining the loss function (`nn.BCEWithLogitsLoss`) and optimizer (`torch.optim.Adam`).\n",
    "*   Calling `train_model`.\n",
    "*   Saving the trained model's state dictionary and the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual Training Snippet (Not for full execution here)\n",
    "\n",
    "Below is a simplified conceptual representation of the training loop logic. For full execution, refer to `train_sentiment_model_pytorch.py` as it includes `wandb` integration and more complete setup.\n",
    "\n",
    "```python\n",
    "# --- Conceptual Training Snippet ---\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# Assume: \n",
    "# example_train_loader, example_tokenizer_for_data are loaded from previous cell\n",
    "# SimpleSentimentModel class is defined\n",
    "\n",
    "if 'example_train_loader' in globals() and 'example_tokenizer_for_data' in globals():\n",
    "    # Hyperparameters (example values)\n",
    "    VOCAB_SIZE_TRAIN = len(example_tokenizer_for_data.word2idx)\n",
    "    EMBEDDING_DIM_TRAIN = 64\n",
    "    HIDDEN_DIM_TRAIN = 32\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 1 # Keep epochs low for notebook demo\n",
    "\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model, loss, optimizer\n",
    "    training_model = SimpleSentimentModel(VOCAB_SIZE_TRAIN, EMBEDDING_DIM_TRAIN, HIDDEN_DIM_TRAIN).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss() # Suitable for binary classification with logits output\n",
    "    optimizer = optim.Adam(training_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    training_model.train() # Set model to training mode\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        progress_bar = tqdm(example_train_loader, desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1) # Ensure labels are float and match output shape\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = training_model(inputs) # Forward pass (get logits)\n",
    "            loss = criterion(outputs, labels) # Calculate loss\n",
    "            loss.backward() # Backward pass (calculate gradients)\n",
    "            optimizer.step() # Update weights\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # Convert logits to probabilities (via sigmoid) and then to binary predictions (0 or 1)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            progress_bar.set_postfix({'batch_loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        epoch_loss = total_loss / len(example_train_loader)\n",
    "        epoch_accuracy = (correct_predictions / total_samples) * 100\n",
    "        print(f\"Epoch {epoch+1} Completed: Avg Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "else:\n",
    "    print(\"Skipping conceptual training snippet as data was not loaded.\")\n",
    "# --- End Conceptual Training Snippet ---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform actual training, you would typically run the `train_sentiment_model_pytorch.py` script. It includes important details like proper dataset handling, Weights & Biases integration for experiment tracking, and saving the final model and tokenizer.\n",
    "\n",
    "Example command to run the script (from your terminal, in the `week5/lecture/` directory):\n",
    "```bash\n",
    "python train_sentiment_model_pytorch.py\n",
    "```\n",
    "This will train the model on the IMDB dataset and save `sentiment_model.pth` containing the model state and tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook has dissected the components of a simple sentiment analysis model built from scratch using PyTorch.\n",
    "\n",
    "Key Takeaways:\n",
    "*   **Custom Tokenization:** We saw how to build a vocabulary and tokenize text, providing control over text preprocessing.\n",
    "*   **PyTorch Model Definition:** Defining a neural network involves creating a class that inherits from `nn.Module`, defining layers in `__init__`, and specifying the data flow in `forward`.\n",
    "*   **Data Handling:** The `datasets` library simplifies loading standard datasets, and PyTorch's `DataLoader` is essential for efficient batching and shuffling during training.\n",
    "*   **Training Loop:** The core PyTorch training loop involves iterating through epochs and batches, performing forward and backward passes, calculating loss, and updating model parameters.\n",
    "\n",
    "Building models from scratch offers a deeper understanding and more flexibility, though it requires more effort than using pre-trained models via pipelines. This foundational knowledge is crucial for more advanced topics like fine-tuning larger pre-trained transformers or implementing novel architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
