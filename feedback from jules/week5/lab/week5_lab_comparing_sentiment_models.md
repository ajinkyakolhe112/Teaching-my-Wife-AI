# Lab 5: Comparing Hugging Face Pipelines and a Custom PyTorch Model for Sentiment Analysis

**Objective:** To give hands-on experience in using both a Hugging Face pipeline and a pre-trained custom PyTorch model for sentiment analysis, and to compare their outputs on the same data.

**Prerequisites:**
*   Python 3.x installed.
*   Libraries: `transformers`, `torch`, `datasets`.
    ```bash
    pip install transformers torch datasets
    # Add scikit-learn if not already installed for other general ML tasks, though not strictly needed for this lab's core
    # pip install scikit-learn 
    ```
*   Ensure the script `simple_sentiment_model_pytorch.py` (from Week 5 lectures) is in your working directory or Python path. This file contains the definitions for `SimpleTokenizer` and `SimpleSentimentModel`.
*   Access to the pre-trained custom model file `sentiment_model.pth`. This file should have been generated by running the `train_sentiment_model_pytorch.py` script from the lectures. **If you do not have this file, you might need to run `python train_sentiment_model_pytorch.py` once to generate it. This training process might take some time.** (For a typical lab setting, this `.pth` file would be provided.)

---

## Tasks:

### 1. Setup

*   **Ensure `simple_sentiment_model_pytorch.py` is available:**
    Download or copy `simple_sentiment_model_pytorch.py` into your current working directory. This file contains the class definitions for `SimpleTokenizer` and `SimpleSentimentModel`.

*   **Obtain `sentiment_model.pth`:**
    This file contains the trained weights of our custom `SimpleSentimentModel` and the `SimpleTokenizer` instance used during its training.
    *   **If provided:** Ensure `sentiment_model.pth` is in your current working directory.
    *   **If you need to generate it:**
        1.  Make sure you have `simple_sentiment_model_pytorch.py` and `train_sentiment_model_pytorch.py` in the same directory.
        2.  Run the training script from your terminal: `python train_sentiment_model_pytorch.py`
        3.  This will train the model (which can take 10-30 minutes or more depending on your hardware and if you're using a GPU) and save `sentiment_model.pth`.
        4.  *For this lab, we assume the `.pth` file is ready to minimize lab time.*

### 2. Prepare Sample Data

Here are some diverse text samples. We will use these to test both models.

```python
sample_texts = [
    "This movie was absolutely fantastic! The acting, a_plot, and cinematography were all top-notch.", # Clearly Positive
    "I am incredibly disappointed with this product. It broke after just one day.", # Clearly Negative
    "The weather today is neither hot nor cold; it's just average.", # Neutral
    "An amazing experience from start to finish, highly recommended!", # Clearly Positive
    "This is the worst service I have ever received. Avoid at all costs.", # Clearly Negative
    "The book was okay, but I found the ending a bit predictable.", # Mixed/Neutral leaning Negative
    "I'm not sure how I feel about this new policy.", # Neutral/Uncertain
    "What a delightful surprise! I wasn't expecting to enjoy it this much.", # Positive
    "Completely underwhelming and a waste of money.", # Negative
    "The lecture was informative, though somewhat dry." # Neutral/Mixed
]
```

### 3. Sentiment Analysis with Hugging Face Pipeline

*   Write Python code to initialize a standard sentiment analysis pipeline from Hugging Face.
*   Run the `sample_texts` through this pipeline.
*   Store the predicted labels and scores.

```python
from transformers import pipeline
import pandas as pd # For displaying results in a table

# Initialize HF pipeline
try:
    hf_pipeline = pipeline("sentiment-analysis")
    print("Hugging Face pipeline loaded successfully.")
except Exception as e:
    print(f"Error loading Hugging Face pipeline: {e}")
    hf_pipeline = None

# Get predictions from HF pipeline
hf_predictions = []
if hf_pipeline:
    print("\\nProcessing with Hugging Face Pipeline...")
    for text in sample_texts:
        try:
            result = hf_pipeline(text)[0] # pipeline returns a list of dicts
            hf_predictions.append({'text': text, 'hf_label': result['label'], 'hf_score': result['score']})
            print(f"Text: \"{text}\" => Label: {result['label']}, Score: {result['score']:.4f}")
        except Exception as e:
            print(f"Error processing text with HF pipeline: \"{text}\" - {e}")
            hf_predictions.append({'text': text, 'hf_label': 'ERROR', 'hf_score': 0.0})
else:
    print("Skipping Hugging Face Pipeline analysis as it failed to load.")

# Create a DataFrame for easier comparison later
results_df = pd.DataFrame(hf_predictions)
```

### 4. Sentiment Analysis with Custom PyTorch Model

*   Write Python code to load your custom `SimpleTokenizer` and `SimpleSentimentModel`.
*   Load the pre-trained weights from `sentiment_model.pth`.
*   Tokenize the `sample_texts` and get predictions from your model.

```python
import torch
import torch.nn.functional as F
# Ensure simple_sentiment_model_pytorch.py is in the same directory or your PYTHONPATH
from simple_sentiment_model_pytorch import SimpleSentimentModel, SimpleTokenizer

# --- Load the custom model and tokenizer ---
custom_model_predictions_list = []
MODEL_PATH = 'sentiment_model.pth'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\\nUsing device for PyTorch model: {device}")

try:
    # Load the saved model state and tokenizer
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    
    # Load tokenizer (it was saved during training)
    custom_tokenizer = checkpoint['tokenizer']
    print("Custom tokenizer loaded successfully.")
    
    # Initialize model with the same vocab_size as during training
    # The vocab_size is implicitly defined by the loaded tokenizer
    custom_model = SimpleSentimentModel(vocab_size=len(custom_tokenizer.word2idx))
    custom_model.load_state_dict(checkpoint['model_state_dict'])
    custom_model.to(device)
    custom_model.eval() # Set model to evaluation mode
    print("Custom PyTorch model loaded and set to evaluation mode.")

    print("\\nProcessing with Custom PyTorch Model...")
    with torch.no_grad(): # Disable gradient calculations for inference
        for text in sample_texts:
            try:
                # Tokenize text
                tokenized_text = custom_tokenizer.tokenize(text).unsqueeze(0).to(device) # Add batch dimension and move to device
                
                # Get model output (logits)
                logits = custom_model(tokenized_text)
                
                # Apply sigmoid to get probability
                probability = torch.sigmoid(logits.squeeze())
                
                # Determine label based on threshold (0.5)
                predicted_label_num = (probability > 0.5).float().item()
                predicted_label_str = "POSITIVE" if predicted_label_num == 1.0 else "NEGATIVE"
                
                custom_model_predictions_list.append({'custom_label': predicted_label_str, 'custom_score': probability.item()})
                print(f"Text: \"{text}\" => Label: {predicted_label_str}, Probability (Positive): {probability.item():.4f}")
            except Exception as e:
                print(f"Error processing text with custom model: \"{text}\" - {e}")
                custom_model_predictions_list.append({'custom_label': 'ERROR', 'custom_score': 0.0})

except FileNotFoundError:
    print(f"ERROR: Model file '{MODEL_PATH}' not found. Please ensure it's generated or provided.")
    # Fill with error placeholders if model loading fails
    for text in sample_texts:
        custom_model_predictions_list.append({'custom_label': 'MODEL_LOAD_ERROR', 'custom_score': 0.0})
except Exception as e:
    print(f"An error occurred while loading or running the custom PyTorch model: {e}")
    for text in sample_texts:
        custom_model_predictions_list.append({'custom_label': 'ERROR', 'custom_score': 0.0})


# --- Add custom model predictions to the DataFrame ---
if results_df.empty and custom_model_predictions_list: # If HF pipeline failed but custom model ran
    results_df = pd.DataFrame(custom_model_predictions_list)
    results_df['text'] = sample_texts # Add text column if not already there
elif custom_model_predictions_list:
    custom_preds_df = pd.DataFrame(custom_model_predictions_list)
    results_df = pd.concat([results_df.reset_index(drop=True), custom_preds_df.reset_index(drop=True)], axis=1)
else: # Both failed or custom model list is empty
    print("\\nCould not generate results from custom PyTorch model.")

```

### 5. Compare Results

*   Display the collected predictions in a table and discuss your observations.

```python
# --- Display the comparison table ---
# Ensure the 'text' column is first if it exists, or add it if only custom_model_predictions were made
if 'text' not in results_df.columns and custom_model_predictions_list:
    # This case might happen if HF pipeline failed and results_df was initially empty
    # and then populated only by custom_model_predictions_list (which doesn't include 'text' directly)
    # This is a fallback; ideally, results_df is initialized with 'text' from hf_predictions
    temp_df_for_text = pd.DataFrame(sample_texts, columns=['text'])
    results_df = pd.concat([temp_df_for_text, results_df], axis=1)
elif 'text' in results_df.columns and results_df['text'].isnull().all() and custom_model_predictions_list:
    results_df['text'] = sample_texts


print("\\n--- Comparison Table ---")
if not results_df.empty:
    # Select and reorder columns for display if they exist
    display_cols = []
    if 'text' in results_df.columns:
        display_cols.append('text')
    if 'hf_label' in results_df.columns:
        display_cols.extend(['hf_label', 'hf_score'])
    if 'custom_label' in results_df.columns:
        display_cols.extend(['custom_label', 'custom_score'])
    
    if display_cols:
        print(results_df[display_cols].to_string())
    else:
        print("No data to display in the results table.")

else:
    print("No results to display. Both models may have failed to process data.")

```

**Discussion Prompts:**

*   Look at the table. For which texts did the models agree? For which did they disagree?
*   The Hugging Face pipeline (default `distilbert-base-uncased-finetuned-sst-2-english`) outputs 'POSITIVE' or 'NEGATIVE'. Our custom PyTorch model also outputs 'POSITIVE' or 'NEGATIVE' based on a 0.5 threshold. How do their raw scores/probabilities compare for nuanced texts?
*   Which model seemed more nuanced, if any? For example, for texts like "The book was okay, but I found the ending a bit predictable," how did each model classify it?
*   Were there any classifications that surprised you from either model?
*   Considering the complexity of setting up each (HF pipeline vs. loading and running the custom model), what are your thoughts on when you might prefer one over the other for a quick sentiment analysis task?

---

**Lab Conclusion:**
This lab provided a practical comparison between using a high-level Hugging Face pipeline and a custom-trained PyTorch model for sentiment analysis. You should have observed differences in setup, output, and potentially classification results. Pre-trained pipelines are often great for general tasks and quick results, while custom models (even simple ones) offer more control and insight into the modeling process. The performance of the custom model heavily depends on its architecture, training data, and training process.
